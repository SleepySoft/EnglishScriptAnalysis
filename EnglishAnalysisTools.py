import re
import nltk
import string
import traceback
import unicodedata
import pandas as pd
from collections import Counter
from typing import Tuple, List, Dict
from nltk import pos_tag
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords, wordnet
from nltk.tokenize import word_tokenize, sent_tokenize


def check_download_nlp_data():
    # ç¡®ä¿å·²ä¸‹è½½æ‰€éœ€çš„NLTKæ•°æ®
    nltk.download('punkt')
    nltk.download('stopwords')
    nltk.download('punkt_tab')
    nltk.download('wordnet')
    nltk.download('averaged_perceptron_tagger')  # ç”¨äºè¯æ€§æ ‡æ³¨ï¼Œå¸®åŠ©è¯å½¢è¿˜åŸ
    nltk.download('averaged_perceptron_tagger_eng')
check_download_nlp_data()       # Execute immediately when module loading


def normalize_punctuation_to_ascii(text):
    """
    å°†å¸¸è§çš„ä¸­æ–‡æ ‡ç‚¹ç¬¦å·è½¬æ¢ä¸ºå¯¹åº”çš„è‹±æ–‡æ ‡ç‚¹ç¬¦å·ã€‚
    æ³¨æ„ï¼šè¿™æ˜¯ä¸€ä¸ªç¤ºä¾‹æ€§çš„æ˜ å°„ï¼Œå¹¶éæ‰€æœ‰æ ‡ç‚¹éƒ½æœ‰ä¸€ä¸€å¯¹åº”å…³ç³»ï¼Œä½ å¯ä»¥æ ¹æ®éœ€è¦æ‰©å……ã€‚

    Args:
        text (str): è¾“å…¥çš„åŸå§‹æ–‡æœ¬ï¼Œå¯èƒ½åŒ…å«ä¸­æ–‡æ ‡ç‚¹ã€‚

    Returns:
        str: è½¬æ¢åçš„æ–‡æœ¬ï¼Œä¸­æ–‡æ ‡ç‚¹è¢«æ›¿æ¢ä¸ºè‹±æ–‡æ ‡ç‚¹ã€‚
    """
    # æ„å»ºä¸€ä¸ªä¸­æ–‡æ ‡ç‚¹åˆ°è‹±æ–‡æ ‡ç‚¹çš„æ˜ å°„å­—å…¸
    punctuation_map = {
        'ï¼Œ': ',',  # ä¸­æ–‡é€—å· -> è‹±æ–‡é€—å·
        'ã€‚': '.',  # ä¸­æ–‡å¥å· -> è‹±æ–‡å¥å·
        'ï¼›': ';',  # ä¸­æ–‡åˆ†å· -> è‹±æ–‡åˆ†å·
        'ï¼š': ':',  # ä¸­æ–‡å†’å· -> è‹±æ–‡å†’å·
        'ï¼Ÿ': '?',  # ä¸­æ–‡é—®å· -> è‹±æ–‡é—®å·
        'ï¼': '!',  # ä¸­æ–‡æ„Ÿå¹å· -> è‹±æ–‡æ„Ÿå¹å·
        'â€œ': '"',  # ä¸­æ–‡åŒå¼•å· -> è‹±æ–‡åŒå¼•å·
        'â€': '"',
        'â€˜': "'",  # ä¸­æ–‡å•å¼•å· -> è‹±æ–‡å•å¼•å·
        'â€™': "'",
        'ï¼ˆ': '(',  # ä¸­æ–‡æ‹¬å· -> è‹±æ–‡æ‹¬å·
        'ï¼‰': ')',
        'ã€': '[',  # ä¸­æ–‡æ–¹æ‹¬å· -> è‹±æ–‡æ–¹æ‹¬å·
        'ã€‘': ']',
        'ã€Š': '<',  # ä¸­æ–‡ä¹¦åå· -> è‹±æ–‡å°–æ‹¬å· (æˆ–é€šå¸¸ä¹Ÿç›´æ¥å»é™¤)
        'ã€‹': '>',
        'ï½': '~',  # ä¸­æ–‡æ³¢æµªå· -> è‹±æ–‡æ³¢æµªå·
        'â€”': '-',  # ä¸­æ–‡ç ´æŠ˜å· -> è‹±æ–‡è¿å­—ç¬¦ (è¿™æ˜¯ä¸€ä¸ªè¿‘ä¼¼æ›¿æ¢)
        'â€¦': '...',  # ä¸­æ–‡çœç•¥å· -> è‹±æ–‡çœç•¥å·
    }

    # é€ä¸ªå­—ç¬¦æ£€æŸ¥å¹¶æ›¿æ¢
    normalized_text = []
    for char in text:
        if char in punctuation_map:
            normalized_text.append(punctuation_map[char])
        else:
            normalized_text.append(char)

    return ''.join(normalized_text)


def full_width_to_ascii(text):
    """
    å°†å…¨è§’å­—æ¯å’Œæ•°å­—è½¬æ¢ä¸ºåŠè§’ï¼ˆASCIIï¼‰å­—æ¯å’Œæ•°å­—ã€‚
    å…¨è§’å­—ç¬¦çš„UnicodeèŒƒå›´é€šå¸¸ä»FF01åˆ°FF5Eï¼ˆå¯¹åº”æ•°å­—å’Œå­—æ¯ï¼‰ï¼Œ
    å®ƒä»¬ä¸åŠè§’ASCIIå­—ç¬¦æœ‰å›ºå®šçš„åç§»é‡ï¼ˆ0xFEE0ï¼‰ã€‚
    æ­¤å‡½æ•°ä¸ä¼šå½±å“æ ‡ç‚¹ã€æ±‰å­—ç­‰å…¶ä»–å­—ç¬¦ã€‚

    Args:
        text (str): è¾“å…¥çš„æ–‡æœ¬ï¼Œå¯èƒ½åŒ…å«å…¨è§’å­—æ¯å’Œæ•°å­—ã€‚

    Returns:
        str: è½¬æ¢åçš„æ–‡æœ¬ï¼Œå…¨è§’å­—æ¯å’Œæ•°å­—è¢«è½¬æ¢ä¸ºåŠè§’ã€‚
    """
    normalized_text = []
    for char in text:
        # è·å–å­—ç¬¦çš„Unicodeåç§°ï¼Œå¸¸ç”¨äºåˆ¤æ–­å­—ç¬¦ç±»å‹
        name = unicodedata.name(char, '')
        # æ£€æŸ¥æ˜¯å¦ä¸ºå…¨è§’å­—ç¬¦ï¼ˆFULLWIDTH ...ï¼‰
        if 'FULLWIDTH' in name:
            # å°è¯•å°†å…¶è½¬æ¢ä¸ºåŠè§’å½¢å¼
            try:
                # ä½¿ç”¨ unicodedata.normalize è½¬æ¢ï¼Œä½†æ›´ç›´æ¥çš„æ˜¯è®¡ç®—å…¶åŠè§’ç ç‚¹
                # å…¨è§’å­—ç¬¦ä¸åŠè§’å­—ç¬¦çš„ç ç‚¹ç›¸å·® 0xFEE0
                half_width_char = chr(ord(char) - 0xFEE0)
                # ç¡®ä¿è½¬æ¢åçš„å­—ç¬¦ç¡®å®æ˜¯ASCIIï¼ˆä¾‹å¦‚ï¼Œå…¨è§’'A'è½¬åŠè§’'A'ï¼‰
                if half_width_char.isascii():
                    normalized_text.append(half_width_char)
                else:
                    # å¦‚æœè½¬æ¢åä¸æ˜¯ASCIIï¼Œä¿ç•™åŸå­—ç¬¦ï¼ˆä¾‹å¦‚æŸäº›å…¨è§’ç¬¦å·ï¼‰
                    normalized_text.append(char)
            except (ValueError, TypeError):
                # å¦‚æœè½¬æ¢å‡ºé”™ï¼Œä¿ç•™åŸå­—ç¬¦
                normalized_text.append(char)
        else:
            # å¦‚æœä¸æ˜¯å…¨è§’å­—ç¬¦ï¼Œç›´æ¥ä¿ç•™
            normalized_text.append(char)
    return ''.join(normalized_text)


def keep_only_ascii(text):
    """
    ç§»é™¤å­—ç¬¦ä¸²ä¸­çš„æ‰€æœ‰éASCIIå­—ç¬¦ã€‚

    Args:
        text (str): å¾…å¤„ç†çš„å­—ç¬¦ä¸²ã€‚

    Returns:
        str: åªåŒ…å«ASCIIå­—ç¬¦çš„å­—ç¬¦ä¸²ã€‚
    """
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…éASCIIå­—ç¬¦ï¼ˆç ç‚¹ > 127 çš„å­—ç¬¦ï¼‰å¹¶ç§»é™¤
    # æ¨¡å¼ [^\x00-\x7F] åŒ¹é…æ‰€æœ‰éASCIIå­—ç¬¦
    ascii_text = re.sub(r'[^\x00-\x7F]', '', text)
    return ascii_text


def remove_digits(text):
    """
    ç§»é™¤å­—ç¬¦ä¸²ä¸­çš„æ‰€æœ‰é˜¿æ‹‰ä¼¯æ•°å­—ã€‚

    Args:
        text (str): å¾…å¤„ç†çš„å­—ç¬¦ä¸²ã€‚

    Returns:
        str: ä¸åŒ…å«æ•°å­—çš„å­—ç¬¦ä¸²ã€‚
    """
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ç§»é™¤éæ•°å­—å­—ç¬¦
    # æ¨¡å¼ [0-9] åŒ¹é…æ‰€æœ‰æ•°å­—
    no_digits_text = re.sub(r'[0-9]', '', text)
    return no_digits_text


def replace_unwanted_symbols(text, keep_chars=""",?!:"'"""):
    """
    å¢å¼ºç‰ˆçš„ç¬¦å·æ›¿æ¢å‡½æ•°ï¼Œå°è¯•åŒºåˆ†å•è¯è¿å­—ç¬¦å’Œæ•°å­¦å‡å·ï¼ŒåŒæ—¶åŒºåˆ†å¥å·å’Œå°æ•°ç‚¹ã€‚

    Args:
        text (str): è¾“å…¥æ–‡æœ¬
        keep_chars (str): é¢å¤–éœ€è¦ä¿ç•™çš„å­—ç¬¦

    Returns:
        str: å¤„ç†åçš„æ–‡æœ¬
    """
    # ä½¿ç”¨ç½•è§å­—ç¬¦ä½œä¸ºä¸´æ—¶æ ‡è®°
    temp_marker_hyphen = "â–¦"  # ç”¨äºå—ä¿æŠ¤çš„è¿å­—ç¬¦
    temp_marker_period = "â—"  # ç”¨äºå—ä¿æŠ¤çš„å¥å·

    # 1. ä¿æŠ¤å•è¯ä¸­çš„è¿å­—ç¬¦ï¼ˆå‰åæ˜¯å­—æ¯ï¼‰
    protected_text = re.sub(r'(?<=[a-zA-Z])-(?=[a-zA-Z])', temp_marker_hyphen, text)

    # 2. ä¿æŠ¤ç–‘ä¼¼å¥å·ï¼ˆä¸åœ¨æ•°å­—é—´ï¼‰
    protected_text = re.sub(r'\.(?!(?<=\d\.)\d)', temp_marker_period, protected_text)

    # 3. å®šä¹‰åŸºç¡€ä¿ç•™é›†åˆï¼ˆå­—æ¯ã€æ•°å­—ã€ç©ºæ ¼ã€ä¸´æ—¶æ ‡è®°ï¼‰
    base_keep = r'\w\s'
    # æ„å»ºä¿ç•™å­—ç¬¦æ¨¡å¼ï¼ˆåŸºç¡€ä¿ç•™ + ç”¨æˆ·æŒ‡å®šä¿ç•™ + ä¸´æ—¶æ ‡è®°ï¼‰
    keep_pattern = f"{base_keep}{re.escape(keep_chars)}{temp_marker_hyphen}{temp_marker_period}"

    # 4. å°†æ‰€æœ‰ä¸åœ¨ä¿ç•™é›†ä¸­çš„å­—ç¬¦æ›¿æ¢ä¸ºç©ºæ ¼
    cleaned_text = re.sub(f"[^{keep_pattern}]", ' ', protected_text)

    # 5. æ¢å¤å—ä¿æŠ¤çš„è¿å­—ç¬¦å’Œå¥å·
    final_text = cleaned_text.replace(temp_marker_hyphen, '-').replace(temp_marker_period, '.')

    return final_text


def reduce_blank_lines(text, max_blanks=2):
    """
    å°†æ–‡æœ¬ä¸­è¿ç»­çš„ç©ºè¡Œå‡å°‘åˆ°æŒ‡å®šæœ€å¤§æ•°é‡ï¼ˆé»˜è®¤ä¿ç•™2ä¸ªï¼‰

    Args:
        text (str): è¾“å…¥çš„æ–‡æœ¬å­—ç¬¦ä¸²
        max_blanks (int): å…è®¸ä¿ç•™çš„æœ€å¤§è¿ç»­ç©ºè¡Œæ•°ï¼Œé»˜è®¤ä¸º2

    Returns:
        str: å¤„ç†åçš„æ–‡æœ¬å­—ç¬¦ä¸²
    """
    # åŒ¹é…è¶…è¿‡max_blanksçš„è¿ç»­ç©ºè¡Œæ¨¡å¼
    # \n\s* åŒ¹é…ä¸€ä¸ªæ¢è¡Œç¬¦åŠå…¶åçš„ä»»æ„ç©ºç™½å­—ç¬¦ï¼ˆåŒ…æ‹¬åç»­æ¢è¡Œç¬¦ï¼‰
    pattern = r'(\n\s*){' + str(max_blanks + 1) + r',}'
    # æ›¿æ¢ä¸ºæ°å¥½max_blanksä¸ªç©ºè¡Œï¼ˆå³max_blanksä¸ªæ¢è¡Œç¬¦ï¼‰
    replacement = '\n' * max_blanks
    # ä½¿ç”¨æ­£åˆ™æ›¿æ¢ï¼Œre.DOTALLç¡®ä¿.åŒ¹é…åŒ…æ‹¬æ¢è¡Œç¬¦åœ¨å†…çš„æ‰€æœ‰å­—ç¬¦
    cleaned_text = re.sub(pattern, replacement, text, flags=re.DOTALL)

    return cleaned_text


def remove_non_english(text, keep_number=False):
    """
    ç§»é™¤å­—ç¬¦ä¸²ä¸­çš„æ‰€æœ‰éè‹±æ–‡å­—ç¬¦ã€‚

    Args:
        text (str): å¾…å¤„ç†çš„åŸå§‹å­—ç¬¦ä¸²ã€‚
        keep_space (bool): æ˜¯å¦ä¿ç•™ç©ºæ ¼å­—ç¬¦ã€‚é»˜è®¤ä¸º Trueã€‚
        keep_number (bool): æ˜¯å¦ä¿ç•™é˜¿æ‹‰ä¼¯æ•°å­—ã€‚é»˜è®¤ä¸º Falseã€‚

    Returns:
        str: åªåŒ…å«è‹±æ–‡å­—æ¯ï¼ˆä»¥åŠå¯é€‰ç©ºæ ¼å’Œæ•°å­—ï¼‰çš„æ¸…æ´—åå­—ç¬¦ä¸²ã€‚

    Raises:
        TypeError: å¦‚æœè¾“å…¥ `text` ä¸æ˜¯å­—ç¬¦ä¸²ç±»å‹ã€‚
    """

    # å‚æ•°æ£€æŸ¥
    if not isinstance(text, str):
        raise TypeError("è¾“å…¥å‚æ•° text å¿…é¡»æ˜¯å­—ç¬¦ä¸²ç±»å‹ (str)")

    step1_text = normalize_punctuation_to_ascii(text)
    step2_text = full_width_to_ascii(step1_text)
    step3_text = keep_only_ascii(step2_text)
    step4_text = replace_unwanted_symbols(step3_text)
    step5_text = remove_digits(step4_text) if not keep_number else step4_text
    return step5_text


def penn_treebank_tag_to_wordnet_tag(treebank_tag):
    """
    å°† Penn Treebank è¯æ€§æ ‡ç­¾è½¬æ¢ä¸º WordNet å…¼å®¹çš„è¯æ€§æ ‡ç­¾ã€‚

    å‚æ•°:
        treebank_tag (str): Penn Treebank è¯æ€§æ ‡ç­¾ã€‚

    è¿”å›:
        str: WordNet è¯æ€§æ ‡ç­¾ (å¦‚ `wn.NOUN`)ï¼Œå¦‚æœæ— æ³•æ˜ å°„åˆ™è¿”å› Noneã€‚
    """
    if treebank_tag.startswith('J'):
        return wordnet.ADJ
    elif treebank_tag.startswith('V'):
        return wordnet.VERB
    elif treebank_tag.startswith('N'):
        return wordnet.NOUN
    elif treebank_tag.startswith('R'):
        return wordnet.ADV
    else:
        # å¯¹äºå…¶ä»–è¯æ€§ï¼ˆå¦‚ä»‹è¯ã€è¿è¯ã€ä»£è¯ç­‰ï¼‰ï¼Œè¿”å› None æˆ–é»˜è®¤å¤„ç†
        return None
ptb_to_wn_tag = penn_treebank_tag_to_wordnet_tag


def get_wordnet_pos_from_sentence(sentence: str, target_word: str):
    """
    åœ¨å¥å­ä¸Šä¸‹æ–‡ä¸­è·å–ç›®æ ‡å•è¯çš„æ‰€æœ‰WordNetè¯æ€§æ ‡ç­¾ã€‚

    Args:
        sentence: åŒ…å«ç›®æ ‡å•è¯çš„å®Œæ•´å¥å­ã€‚
        target_word: éœ€è¦è·å–è¯æ€§çš„ç›®æ ‡å•è¯ã€‚

    Returns:
        list: ä¸€ä¸ªåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªå…ƒç»„ï¼ŒåŒ…å«åŒ¹é…å•è¯çš„ç´¢å¼•ã€å•è¯æœ¬èº«å’Œå…¶WordNetè¯æ€§æ ‡ç­¾ã€‚
              ä¾‹å¦‚ï¼š[(0, 'Can', 'v'), (2, 'can', 'v'), (4, 'can', 'n')]
    """
    words = word_tokenize(sentence)
    pos_tagged = pos_tag(words)  # å¾—åˆ°Penn Treebankæ ‡ç­¾
    results = []

    for index, (word, ptb_tag) in enumerate(pos_tagged):
        if word.lower() == target_word.lower():
            wn_tag = ptb_to_wn_tag(ptb_tag)
            results.append((index, word, wn_tag))

    return results


def is_valid_word(word: str, min_length: int = 2) -> bool:
    """
    æ£€æŸ¥ä¸€ä¸ªå­—ç¬¦ä¸²æ˜¯å¦ä¸ºæœ‰æ•ˆçš„å•è¯ï¼ˆè¿‡æ»¤æ•°å­—ã€çº¯ç¬¦å·ç­‰ï¼‰ã€‚

    Args:
        word (str): å¾…æ£€æŸ¥çš„å­—ç¬¦ä¸²ã€‚
        min_length (int): å•è¯çš„æœ€å°æœ‰æ•ˆé•¿åº¦ã€‚

    Returns:
        bool: å¦‚æœæ˜¯æœ‰æ•ˆå•è¯åˆ™è¿”å› Trueï¼Œå¦åˆ™è¿”å› Falseã€‚
    """
    if not word:
        return False
    if len(word) < min_length:
        return False
    # è¿‡æ»¤æ‰åŒ…å«æ•°å­—çš„å­—ç¬¦ä¸²
    if re.search(r'\d', word):
        return False
    # å¯ä»¥æ·»åŠ å…¶ä»–è¿‡æ»¤è§„åˆ™ï¼Œä¾‹å¦‚è¿‡æ»¤æ‰çº¯ç¬¦å·ï¼ˆä½†ç»è¿‡é¢„å¤„ç†åé€šå¸¸ä¸ä¼šå‡ºç°ï¼‰
    return True


def get_top_words(word_freq: Dict[str, int], n: int = 10) -> List[Tuple[str, int]]:
    """
    è·å–å‰Nä¸ªæœ€å¸¸å‡ºç°çš„å•è¯ã€‚

    Args:
        word_freq (Dict[str, int]): è¯é¢‘å­—å…¸ã€‚
        n (int): è¦è·å–çš„é¡¶éƒ¨å•è¯æ•°é‡ï¼Œé»˜è®¤ä¸º10ã€‚

    Returns:
        List[Tuple[str, int]]: å‰Nä¸ªå•è¯åŠå…¶é¢‘ç‡çš„åˆ—è¡¨ã€‚
    """
    return Counter(word_freq).most_common(n)


def analyze_collocations(text, top_n=20):
    """
    åˆ†æå¸¸è§çš„è¯æ€§æ­é…æ¨¡å¼ï¼Œè¿™æœ‰åŠ©äºå‘ç°è‹±è¯­ä¸­çš„ä¹ æƒ¯ç”¨æ³•
    ä¾‹å¦‚ï¼šåŠ¨è¯+ä»‹è¯ï¼ˆVB+INï¼‰ã€å½¢å®¹è¯+åè¯ï¼ˆJJ+NNï¼‰ç­‰ã€‚
    """

    tokens = word_tokenize(text)
    tagged_tokens = pos_tag(tokens)

    # å®šä¹‰ä¸€äº›å¸¸è§çš„ã€æœ‰æ„ä¹‰çš„è¯æ€§ç»„åˆæ¨¡å¼

    patterns = [
        # åŸºç¡€æ¨¡å¼
        (('VB', 'IN'), 'Verb+Prep'),  # åŠ¨è¯+ä»‹è¯ï¼Œå¦‚ï¼šlook at, depend on, talk about
        (('VB', 'DT', 'NN'), 'Verb+Det+Noun'),  # åŠ¨è¯+é™å®šè¯+åè¯ï¼Œå¦‚ï¼šhave a look, make a decision, take the chance
        (('JJ', 'NN'), 'Adj+Noun'),  # å½¢å®¹è¯+åè¯ï¼Œå¦‚ï¼šred apple, important meeting, difficult situation
        (('RB', 'VB'), 'Adv+Verb'),  # å‰¯è¯+åŠ¨è¯ï¼Œå¦‚ï¼šquickly run, easily understand, carefully consider
        (('NN', 'IN', 'NN'), 'Noun+Prep+Noun'),  # åè¯+ä»‹è¯+åè¯ï¼Œå¦‚ï¼štransition to adulthood, key to success, fear of failure
        (('VB', 'RB'), 'Verb+Adv'),  # åŠ¨è¯+å‰¯è¯ï¼Œå¦‚ï¼šspeak clearly, work efficiently, respond immediately
        (('IN', 'DT', 'NN'), 'Prep+Det+Noun'),  # ä»‹è¯+é™å®šè¯+åè¯ï¼Œå¦‚ï¼šin the morning, on a mission, with an idea
        (('NN', 'NN'), 'Compound Noun'),  # å¤åˆåè¯ï¼Œå¦‚: coffee cup, business meeting, research paper
        (('VB', 'NN'), 'Verb+Noun'),  # åŠ¨è¯+åè¯ï¼Œå¦‚: make progress, take notes, set goals

        # æ–°å¢æ¨¡å¼
        (('VB', 'DT', 'JJ', 'NN'), 'Verb+Det+Adj+Noun'), # åŠ¨è¯+é™å®šè¯+å½¢å®¹è¯+åè¯ï¼Œå¦‚ï¼šhave a great day, make an important decision, see the beautiful sunset
        (('JJ', 'JJ', 'NN'), 'Adj+Adj+Noun'),  # å½¢å®¹è¯+å½¢å®¹è¯+åè¯ï¼Œå¦‚ï¼šbeautiful red rose, large wooden table, small black cat
        (('RB', 'JJ'), 'Adv+Adj'),  # å‰¯è¯+å½¢å®¹è¯ï¼Œå¦‚ï¼šextremely important, very happy, quite difficult
        (('NN', 'VB'), 'Noun+Verb'),  # åè¯+åŠ¨è¯ï¼Œå¦‚ï¼šproblem solving, decision making, time management
        (('VB', 'PRP'), 'Verb+Pronoun'),  # åŠ¨è¯+ä»£è¯ï¼Œå¦‚ï¼šhelp me, tell them, ask us
        (('IN', 'JJ', 'NN'), 'Prep+Adj+Noun'),  # ä»‹è¯+å½¢å®¹è¯+åè¯ï¼Œå¦‚ï¼šin great detail, with special care, on important matters
        (('DT', 'NN', 'IN', 'NN'), 'Det+Noun+Prep+Noun'), # é™å®šè¯+åè¯+ä»‹è¯+åè¯ï¼Œå¦‚ï¼šthe end of time, a piece of cake, the beginning of history
        (('MD', 'VB', 'RB'), 'Modal+Verb+Adv'), # æƒ…æ€åŠ¨è¯+åŠ¨è¯+å‰¯è¯ï¼Œå¦‚ï¼šcan easily do, will quickly go, should carefully consider
        (('NN', 'IN', 'DT', 'NN'), 'Noun+Prep+Det+Noun'), # åè¯+ä»‹è¯+é™å®šè¯+åè¯ï¼Œå¦‚ï¼štransition to a new, solution to the problem, key to a mystery
        (('VB', 'TO', 'VB'), 'Verb+To+Verb'),  # åŠ¨è¯+ä¸å®šå¼æ ‡è®°+åŠ¨è¯ï¼Œå¦‚ï¼šwant to go, need to see, try to understand
        (('VBG', 'NN'), 'Gerund+Noun'),  # åŠ¨åè¯+åè¯ï¼Œå¦‚ï¼šreading books, making progress, writing letters
        (('VBN', 'IN'), 'PastPart+Prep'),  # è¿‡å»åˆ†è¯+ä»‹è¯ï¼Œå¦‚ï¼šinterested in, covered with, known for
        (('CD', 'NNS'), 'Number+PluralNoun'),  # åŸºæ•°è¯+åè¯å¤æ•°ï¼Œå¦‚ï¼šthree books, five years, ten students
        (('JJ', 'CC', 'JJ'), 'Adj+Conj+Adj'),  # å½¢å®¹è¯+è¿è¯+å½¢å®¹è¯ï¼Œå¦‚ï¼šsimple and effective, short but clear, tired yet happy
        (('VB', 'PRP', 'RB'), 'Verb+Pronoun+Adv'),  # åŠ¨è¯+ä»£è¯+å‰¯è¯ï¼Œå¦‚ï¼štell me quickly, show them clearly, ask us politely
        (('RB', 'RB', 'JJ'), 'Adv+Adv+Adj'), # å‰¯è¯+å‰¯è¯+å½¢å®¹è¯ï¼Œå¦‚ï¼švery extremely hot, quite surprisingly good, rather unexpectedly cold
        (('DT', 'JJ', 'NN', 'VBZ'), 'Det+Adj+Noun+Verb'), # é™å®šè¯+å½¢å®¹è¯+åè¯+åŠ¨è¯ï¼Œå¦‚ï¼šthe quick brown fox jumps, a beautiful red rose blooms
        (('PRP', 'MD', 'VB', 'RB'), 'Pron+Modal+Verb+Adv'), # ä»£è¯+æƒ…æ€åŠ¨è¯+åŠ¨è¯+å‰¯è¯ï¼Œå¦‚ï¼šI can easily do, you should carefully consider, we will quickly go
        (('NN', 'VBZ', 'JJ'), 'Noun+Verb+Adj'),  # åè¯+åŠ¨è¯+å½¢å®¹è¯ï¼Œå¦‚: time flies fast, sun sets red, water runs clear
        (('IN', 'PRP$', 'NN'), 'Prep+Possessive+Noun')  # ä»‹è¯+ç‰©ä¸»ä»£è¯+åè¯ï¼Œå¦‚ï¼šin my opinion, on his behalf, with her permission
    ]

    collocation_counts = {desc: Counter() for _, desc in patterns}
    window_size = 4  # æ£€æŸ¥ç›¸é‚»å•è¯çš„çª—å£å¤§å°

    for i in range(int(len(tagged_tokens) - window_size + 1)):
        window = tagged_tokens[i:i + window_size]
        for (pattern, description) in patterns:
            if len(pattern) <= len(window):
                match = True
                for j in range(len(pattern)):
                    if window[j][1] != pattern[j]:
                        match = False
                        break
                if match:
                    phrase = ' '.join([word for word, pos in window[:len(pattern)]])
                    collocation_counts[description][phrase] += 1

    # è·å–æ¯ç§æ¨¡å¼çš„å‰top_nä¸ªæœ€å¸¸è§æ­é…
    top_collocations = {}
    for desc, counter in collocation_counts.items():
        top_collocations[desc] = counter.most_common(top_n)

    return top_collocations


def count_word_frequency(text: str,
                         remove_stopwords: bool = True,
                         min_word_length: int = 2,
                         lemmatize: bool = True) -> Tuple[List[str], Dict[str, int]]:
    """
    ç»Ÿè®¡æ–‡æœ¬ä¸­å•è¯çš„é¢‘ç‡ï¼Œå¹¶è¿›è¡Œè¯¦ç»†çš„é¢„å¤„ç†ã€‚

    Args:
        text (str): è¦åˆ†æçš„æ–‡æœ¬ã€‚
        remove_stopwords (bool): æ˜¯å¦ç§»é™¤åœç”¨è¯ï¼Œé»˜è®¤ä¸º Trueã€‚
        min_word_length (int): å•è¯æœ€å°é•¿åº¦ï¼ŒçŸ­äºæ­¤é•¿åº¦çš„å•è¯å°†è¢«è¿‡æ»¤ï¼Œé»˜è®¤ä¸º 2ã€‚
        lemmatize (bool): æ˜¯å¦è¿›è¡Œè¯å½¢è¿˜åŸï¼Œé»˜è®¤ä¸º Trueã€‚

    Returns:
        Tuple[List[str], Dict[str, int]]: å¥å­åˆ—è¡¨å’Œå•è¯é¢‘ç‡å­—å…¸ã€‚

    Raises:
        ValueError: å½“è¾“å…¥æ–‡æœ¬ä¸ºç©ºæˆ–è¿‡çŸ­æ—¶ã€‚
    """

    # å‚æ•°éªŒè¯
    if not text or not isinstance(text, str):
        raise ValueError("è¾“å…¥æ–‡æœ¬å¿…é¡»æ˜¯éç©ºå­—ç¬¦ä¸²")
    if len(text.strip()) < 10:  # å‡è®¾æ–‡æœ¬è‡³å°‘10ä¸ªå­—ç¬¦
        raise ValueError("è¾“å…¥æ–‡æœ¬è¿‡çŸ­ï¼Œæ— æ³•è¿›è¡Œæœ‰æ„ä¹‰çš„åˆ†æ")

    # 0. å¯é€‰ï¼šåˆæ­¥æ¸…ç†æ–‡æœ¬ï¼ˆç§»é™¤å¤šä½™ç©ºæ ¼ã€æ¢è¡Œç­‰ï¼‰
    clean_text = re.sub(r'\s+', ' ', text.strip())  # å°†å¤šä¸ªç©ºç™½å­—ç¬¦æ›¿æ¢ä¸ºå•ä¸ªç©ºæ ¼

    # 1. åˆ†å¥
    try:
        sentences = sent_tokenize(clean_text)
    except Exception as e:
        raise RuntimeError(f"åˆ†å¥å¤„ç†å¤±è´¥: {str(e)}")

    # åˆå§‹åŒ–å·¥å…·
    stop_words = set(stopwords.words('english')) if remove_stopwords else set()
    lemmatizer = WordNetLemmatizer() if lemmatize else None
    # åˆ›å»ºå»é™¤æ ‡ç‚¹çš„ç¿»è¯‘è¡¨
    translator = str.maketrans('', '', string.punctuation)

    all_words = []

    for sentence in sentences:
        try:
            # 2. åˆ†è¯
            words = word_tokenize(sentence)

            processed_words = []
            for word in words:
                # 2.1 è½¬æ¢ä¸ºå°å†™
                word_lower = word.lower()
                # 2.2 å»é™¤æ ‡ç‚¹ç¬¦å·ï¼ˆä½¿ç”¨translateæ–¹æ³•ï¼Œæ¯”å¾ªç¯åˆ¤æ–­æ•ˆç‡é«˜ï¼‰[10,11](@ref)
                word_no_punct = word_lower.translate(translator)
                # 2.3 æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆå•è¯ï¼ˆé•¿åº¦ã€æ˜¯å¦åŒ…å«æ•°å­—ç­‰ï¼‰
                if not is_valid_word(word_no_punct, min_word_length):
                    continue
                # 2.4 æ£€æŸ¥åœç”¨è¯
                if remove_stopwords and word_no_punct in stop_words:
                    continue

                processed_words.append(word_no_punct)

            # å¦‚æœå½“å‰å¥å­ç»è¿‡è¿‡æ»¤åæ²¡æœ‰è¯ï¼Œåˆ™è·³è¿‡åç»­å¤„ç†
            if not processed_words:
                continue

            # 3. è¯æ€§æ ‡æ³¨ä¸è¯å½¢è¿˜åŸ (å¦‚æœéœ€è¦)
            if lemmatize and lemmatizer:
                # å¯¹å¤„ç†åçš„å•è¯è¿›è¡Œè¯æ€§æ ‡æ³¨ï¼ˆæ³¨æ„ï¼šè¿™é‡Œæ ‡æ³¨çš„æ˜¯åŸå§‹clean wordï¼Œä½†å®é™…ç”¨çš„æ˜¯è½¬å°å†™åçš„ï¼Œç•¥æœ‰è¯¯å·®ä½†å¯æ¥å—ï¼‰
                pos_tags = pos_tag(processed_words) # è¿”å›å½¢å¼å¦‚ [('word', 'tag'), ...]
                final_words = []
                for word, tag in pos_tags:
                    # è·å–WordNetè¯æ€§
                    wn_tag = ptb_to_wn_tag(tag)
                    if wn_tag:
                        # è¿›è¡Œè¯å½¢è¿˜åŸ
                        lemma = lemmatizer.lemmatize(word, pos=wn_tag)
                        final_words.append(lemma)
                    else:
                        final_words.append(word)
            else:
                final_words = processed_words # ä¸è¿›è¡Œè¯å½¢è¿˜åŸ

            all_words.extend(final_words)

        except Exception as e:
            print(f"å¤„ç†å¥å­æ—¶å‡ºé”™: '{sentence}'. é”™è¯¯: {str(e)}")
            traceback.print_exc()
            continue

    # 4. ç»Ÿè®¡è¯é¢‘
    word_freq = Counter(all_words)

    return sentences, dict(word_freq)

# ----------------------------------------------------------------------------------------------------------------------

def demo_remove_non_english():
    # æµ‹è¯•ç”¨ä¾‹
    test_text = "Hello, ä½ å¥½ï¼ This is a test. 123 456 ğŸ‰"

    # ç¤ºä¾‹ 1: é»˜è®¤æ¨¡å¼ï¼ˆåªä¿ç•™å­—æ¯å’Œç©ºæ ¼ï¼‰
    result1 = remove_non_english(test_text)
    print("é»˜è®¤æ¨¡å¼ (ä¿ç•™å­—æ¯å’Œç©ºæ ¼):", result1)  # è¾“å‡º: "Hello  This is a test  "

    # ç¤ºä¾‹ 2: ä¿ç•™å­—æ¯å’Œæ•°å­—
    result2 = remove_non_english(test_text, keep_number=True)
    print("ä¿ç•™å­—æ¯å’Œæ•°å­—:", result2)  # è¾“å‡º: "HelloThisisatest123456"


def demo_replace_unwanted_symbols():
    test_cases = [
        # åŸºç¡€æµ‹è¯•ï¼šä¿ç•™å­—æ¯ã€æ•°å­—ã€ç©ºæ ¼
        ('Hello World 123', '', 'Hello World 123'),
        # è¿å­—ç¬¦æµ‹è¯•1ï¼šå•è¯ä¸­çš„è¿å­—ç¬¦åº”ä¿ç•™
        ('bi-directional optimization', '', 'bi-directional optimization'),
        # è¿å­—ç¬¦æµ‹è¯•2ï¼šæ•°å­¦è¡¨è¾¾å¼ä¸­çš„å‡å·åº”æ›¿æ¢ä¸ºç©ºæ ¼
        ('3 - 2 result is 1', '', '3   2 result is 1'),
        ('3-2 result is 1', '', '3 2 result is 1'),
        # å¥ç‚¹æµ‹è¯•1ï¼šå¥å·åº”ä¿ç•™
        ('This is a sentence. Another one.', '', 'This is a sentence. Another one.'),
        # å¥ç‚¹æµ‹è¯•2ï¼šå°æ•°ç‚¹åº”æ›¿æ¢ä¸ºç©ºæ ¼
        ('The value is 3.14', '', 'The value is 3 14'),
        # æ··åˆæµ‹è¯•1ï¼šåŒæ—¶åŒ…å«å•è¯è¿å­—ç¬¦ã€æ•°å­¦å‡å·ã€å¥å·å’Œå°æ•°ç‚¹
        ('pi is approx 3.14. pre-defined value: 5 - 3 = 2.', ':', 'pi is approx 3 14. pre-defined value: 5   3   2.'),
        # è‡ªå®šä¹‰ä¿ç•™å­—ç¬¦æµ‹è¯•1ï¼šä¿ç•™@ç¬¦å·
        ('Email me at user@example.com', '@', 'Email me at user@example.com'),
        # è‡ªå®šä¹‰ä¿ç•™å­—ç¬¦æµ‹è¯•2ï¼šä¿ç•™é€—å·å’Œé—®å·
        ('Hello, world! How are you?', ',?', 'Hello, world  How are you?'),
        # è¾¹ç•Œæµ‹è¯•1ï¼šå­—ç¬¦ä¸²å¼€å¤´å’Œç»“å°¾çš„ç¬¦å·
        ('!@#$Hello%^&*', '', '    Hello    '),
        # è¾¹ç•Œæµ‹è¯•2ï¼šç©ºå­—ç¬¦ä¸²
        ('', '', ''),
        # è¾¹ç•Œæµ‹è¯•3ï¼šåªæœ‰ç¬¦å·
        ('!@#$%^&*', '', '        '),
        # å¤æ‚ç¬¦å·æµ‹è¯•ï¼šå¤šç§ç¬¦å·æ··åˆ
        ('a-b c-d e.f g,h i;j k|l', '', 'a-b c-d e.f g h i j k l'),
        # æ•°å­—å’Œç¬¦å·ç»„åˆæµ‹è¯•
        ('1-2-3 4.5 6,7 8:9', '', '1 2 3 4 5 6 7 8 9'),
        # ä¿ç•™å­—ç¬¦ä¸­çš„è¿å­—ç¬¦å¤„ç†ï¼ˆå¦‚æœkeep_charsä¸­åŒ…å«'-'ï¼Œä¸”ä½äºå­—ç¬¦é›†æœ«å°¾ï¼‰
        ('This-is-a-test-string', '-', 'This-is-a-test-string'),
        # ä¿ç•™å­—ç¬¦ä¸­çš„å¥ç‚¹å¤„ç†ï¼ˆå¦‚æœkeep_charsä¸­åŒ…å«'.'ï¼‰
        ('Version.1.2.3', '.', 'Version.1.2.3'),
    ]

    print("å¼€å§‹æµ‹è¯• replace_unwanted_symbols å‡½æ•°ï¼š")
    print("=" * 60)

    for i, (input_text, keep_chars, expected_output) in enumerate(test_cases, 1):
        result = replace_unwanted_symbols(input_text, keep_chars)
        print(f"æµ‹è¯•ç”¨ä¾‹ {i}:")
        print(f"  è¾“å…¥æ–‡æœ¬: '{input_text}'")
        print(f"  ä¿ç•™å­—ç¬¦: '{keep_chars}'")
        print(f"  æœŸæœ›è¾“å‡º: '{expected_output}'")
        print(f"  å®é™…è¾“å‡º: '{result}'")
        print(f"  æ˜¯å¦é€šè¿‡: {result == expected_output}")
        print("-" * 40)


def main():
    demo_remove_non_english()
    demo_replace_unwanted_symbols()


if __name__ == '__main__':
    try:
        main()
    except Exception as e:
        print(str(e))
        traceback.print_exc()
    finally:
        pass
