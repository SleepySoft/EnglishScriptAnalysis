import re
import nltk
import string
import traceback
import pandas as pd
from collections import Counter
from typing import Tuple, List, Dict
from nltk import pos_tag
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords, wordnet
from nltk.tokenize import word_tokenize, sent_tokenize


def check_download_nlp_data():
    # ç¡®ä¿å·²ä¸‹è½½æ‰€éœ€çš„NLTKæ•°æ®
    nltk.download('punkt')
    nltk.download('stopwords')
    nltk.download('punkt_tab')
    nltk.download('wordnet')
    nltk.download('averaged_perceptron_tagger')  # ç”¨äºè¯æ€§æ ‡æ³¨ï¼Œå¸®åŠ©è¯å½¢è¿˜åŸ
    nltk.download('averaged_perceptron_tagger_eng')
check_download_nlp_data()       # Execute immediately when module loading


def remove_non_english(text, keep_space=True, keep_number=False):
    """
    ç§»é™¤å­—ç¬¦ä¸²ä¸­çš„æ‰€æœ‰éè‹±æ–‡å­—ç¬¦ã€‚

    Args:
        text (str): å¾…å¤„ç†çš„åŸå§‹å­—ç¬¦ä¸²ã€‚
        keep_space (bool): æ˜¯å¦ä¿ç•™ç©ºæ ¼å­—ç¬¦ã€‚é»˜è®¤ä¸º Trueã€‚
        keep_number (bool): æ˜¯å¦ä¿ç•™é˜¿æ‹‰ä¼¯æ•°å­—ã€‚é»˜è®¤ä¸º Falseã€‚

    Returns:
        str: åªåŒ…å«è‹±æ–‡å­—æ¯ï¼ˆä»¥åŠå¯é€‰ç©ºæ ¼å’Œæ•°å­—ï¼‰çš„æ¸…æ´—åå­—ç¬¦ä¸²ã€‚

    Raises:
        TypeError: å¦‚æœè¾“å…¥ `text` ä¸æ˜¯å­—ç¬¦ä¸²ç±»å‹ã€‚
    """

    # å‚æ•°æ£€æŸ¥
    if not isinstance(text, str):
        raise TypeError("è¾“å…¥å‚æ•° text å¿…é¡»æ˜¯å­—ç¬¦ä¸²ç±»å‹ (str)")

    # æ ¹æ®å‚æ•°æ„å»ºæ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
    # åŸºç¡€æ¨¡å¼ï¼šåŒ¹é…æ‰€æœ‰è‹±æ–‡å­—æ¯ï¼ˆå¤§å°å†™ï¼‰
    base_pattern = r'a-zA-Z'

    # å¯é€‰ï¼šåœ¨æ¨¡å¼ä¸­æ·»åŠ ç©ºæ ¼
    if keep_space:
        base_pattern += r' '

    # å¯é€‰ï¼šåœ¨æ¨¡å¼ä¸­æ·»åŠ æ•°å­—
    if keep_number:
        base_pattern += r'0-9'

    # åˆ›å»ºæ­£åˆ™è¡¨è¾¾å¼ï¼ŒåŒ¹é…æ‰€æœ‰ä¸åœ¨æŒ‡å®šé›†åˆä¸­çš„å­—ç¬¦
    # æ¨¡å¼ [^...] è¡¨ç¤ºåŒ¹é…ä»»ä½•ä¸åœ¨æ–¹æ‹¬å·å†…çš„å­—ç¬¦
    pattern = re.compile(f'[^{base_pattern}]')

    # ä½¿ç”¨ç©ºå­—ç¬¦ä¸²æ›¿æ¢æ‰€æœ‰éè‹±æ–‡å­—ç¬¦ï¼ˆä»¥åŠæ ¹æ®é€‰æ‹©ä¸ä¿ç•™çš„æ•°å­—å’Œç©ºæ ¼ï¼‰
    cleaned_text = re.sub(pattern, '', text)

    return cleaned_text


def penn_treebank_tag_to_wordnet_tag(treebank_tag):
    """
    å°† Penn Treebank è¯æ€§æ ‡ç­¾è½¬æ¢ä¸º WordNet å…¼å®¹çš„è¯æ€§æ ‡ç­¾ã€‚

    å‚æ•°:
        treebank_tag (str): Penn Treebank è¯æ€§æ ‡ç­¾ã€‚

    è¿”å›:
        str: WordNet è¯æ€§æ ‡ç­¾ (å¦‚ `wn.NOUN`)ï¼Œå¦‚æœæ— æ³•æ˜ å°„åˆ™è¿”å› Noneã€‚
    """
    if treebank_tag.startswith('J'):
        return wordnet.ADJ
    elif treebank_tag.startswith('V'):
        return wordnet.VERB
    elif treebank_tag.startswith('N'):
        return wordnet.NOUN
    elif treebank_tag.startswith('R'):
        return wordnet.ADV
    else:
        # å¯¹äºå…¶ä»–è¯æ€§ï¼ˆå¦‚ä»‹è¯ã€è¿è¯ã€ä»£è¯ç­‰ï¼‰ï¼Œè¿”å› None æˆ–é»˜è®¤å¤„ç†
        return None
ptb_to_wn_tag = penn_treebank_tag_to_wordnet_tag


def get_wordnet_pos_from_sentence(sentence: str, target_word: str):
    """
    åœ¨å¥å­ä¸Šä¸‹æ–‡ä¸­è·å–ç›®æ ‡å•è¯çš„æ‰€æœ‰WordNetè¯æ€§æ ‡ç­¾ã€‚

    Args:
        sentence: åŒ…å«ç›®æ ‡å•è¯çš„å®Œæ•´å¥å­ã€‚
        target_word: éœ€è¦è·å–è¯æ€§çš„ç›®æ ‡å•è¯ã€‚

    Returns:
        list: ä¸€ä¸ªåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªå…ƒç»„ï¼ŒåŒ…å«åŒ¹é…å•è¯çš„ç´¢å¼•ã€å•è¯æœ¬èº«å’Œå…¶WordNetè¯æ€§æ ‡ç­¾ã€‚
              ä¾‹å¦‚ï¼š[(0, 'Can', 'v'), (2, 'can', 'v'), (4, 'can', 'n')]
    """
    words = word_tokenize(sentence)
    pos_tagged = pos_tag(words)  # å¾—åˆ°Penn Treebankæ ‡ç­¾
    results = []

    for index, (word, ptb_tag) in enumerate(pos_tagged):
        if word.lower() == target_word.lower():
            wn_tag = ptb_to_wn_tag(ptb_tag)
            results.append((index, word, wn_tag))

    return results


def is_valid_word(word: str, min_length: int = 2) -> bool:
    """
    æ£€æŸ¥ä¸€ä¸ªå­—ç¬¦ä¸²æ˜¯å¦ä¸ºæœ‰æ•ˆçš„å•è¯ï¼ˆè¿‡æ»¤æ•°å­—ã€çº¯ç¬¦å·ç­‰ï¼‰ã€‚

    Args:
        word (str): å¾…æ£€æŸ¥çš„å­—ç¬¦ä¸²ã€‚
        min_length (int): å•è¯çš„æœ€å°æœ‰æ•ˆé•¿åº¦ã€‚

    Returns:
        bool: å¦‚æœæ˜¯æœ‰æ•ˆå•è¯åˆ™è¿”å› Trueï¼Œå¦åˆ™è¿”å› Falseã€‚
    """
    if not word:
        return False
    if len(word) < min_length:
        return False
    # è¿‡æ»¤æ‰åŒ…å«æ•°å­—çš„å­—ç¬¦ä¸²
    if re.search(r'\d', word):
        return False
    # å¯ä»¥æ·»åŠ å…¶ä»–è¿‡æ»¤è§„åˆ™ï¼Œä¾‹å¦‚è¿‡æ»¤æ‰çº¯ç¬¦å·ï¼ˆä½†ç»è¿‡é¢„å¤„ç†åé€šå¸¸ä¸ä¼šå‡ºç°ï¼‰
    return True


def get_top_words(word_freq: Dict[str, int], n: int = 10) -> List[Tuple[str, int]]:
    """
    è·å–å‰Nä¸ªæœ€å¸¸å‡ºç°çš„å•è¯ã€‚

    Args:
        word_freq (Dict[str, int]): è¯é¢‘å­—å…¸ã€‚
        n (int): è¦è·å–çš„é¡¶éƒ¨å•è¯æ•°é‡ï¼Œé»˜è®¤ä¸º10ã€‚

    Returns:
        List[Tuple[str, int]]: å‰Nä¸ªå•è¯åŠå…¶é¢‘ç‡çš„åˆ—è¡¨ã€‚
    """
    return Counter(word_freq).most_common(n)


def analyze_collocations(text, top_n=20):
    """
    åˆ†æå¸¸è§çš„è¯æ€§æ­é…æ¨¡å¼ï¼Œè¿™æœ‰åŠ©äºå‘ç°è‹±è¯­ä¸­çš„ä¹ æƒ¯ç”¨æ³•
    ä¾‹å¦‚ï¼šåŠ¨è¯+ä»‹è¯ï¼ˆVB+INï¼‰ã€å½¢å®¹è¯+åè¯ï¼ˆJJ+NNï¼‰ç­‰ã€‚
    """

    tokens = word_tokenize(text)
    tagged_tokens = pos_tag(tokens)

    # å®šä¹‰ä¸€äº›å¸¸è§çš„ã€æœ‰æ„ä¹‰çš„è¯æ€§ç»„åˆæ¨¡å¼

    patterns = [
        # åŸºç¡€æ¨¡å¼
        (('VB', 'IN'), 'Verb+Prep'),  # åŠ¨è¯+ä»‹è¯ï¼Œå¦‚ï¼šlook at, depend on, talk about
        (('VB', 'DT', 'NN'), 'Verb+Det+Noun'),  # åŠ¨è¯+é™å®šè¯+åè¯ï¼Œå¦‚ï¼šhave a look, make a decision, take the chance
        (('JJ', 'NN'), 'Adj+Noun'),  # å½¢å®¹è¯+åè¯ï¼Œå¦‚ï¼šred apple, important meeting, difficult situation
        (('RB', 'VB'), 'Adv+Verb'),  # å‰¯è¯+åŠ¨è¯ï¼Œå¦‚ï¼šquickly run, easily understand, carefully consider
        (('NN', 'IN', 'NN'), 'Noun+Prep+Noun'),  # åè¯+ä»‹è¯+åè¯ï¼Œå¦‚ï¼štransition to adulthood, key to success, fear of failure
        (('VB', 'RB'), 'Verb+Adv'),  # åŠ¨è¯+å‰¯è¯ï¼Œå¦‚ï¼šspeak clearly, work efficiently, respond immediately
        (('IN', 'DT', 'NN'), 'Prep+Det+Noun'),  # ä»‹è¯+é™å®šè¯+åè¯ï¼Œå¦‚ï¼šin the morning, on a mission, with an idea
        (('NN', 'NN'), 'Compound Noun'),  # å¤åˆåè¯ï¼Œå¦‚: coffee cup, business meeting, research paper
        (('VB', 'NN'), 'Verb+Noun'),  # åŠ¨è¯+åè¯ï¼Œå¦‚: make progress, take notes, set goals

        # æ–°å¢æ¨¡å¼
        (('VB', 'DT', 'JJ', 'NN'), 'Verb+Det+Adj+Noun'), # åŠ¨è¯+é™å®šè¯+å½¢å®¹è¯+åè¯ï¼Œå¦‚ï¼šhave a great day, make an important decision, see the beautiful sunset
        (('JJ', 'JJ', 'NN'), 'Adj+Adj+Noun'),  # å½¢å®¹è¯+å½¢å®¹è¯+åè¯ï¼Œå¦‚ï¼šbeautiful red rose, large wooden table, small black cat
        (('RB', 'JJ'), 'Adv+Adj'),  # å‰¯è¯+å½¢å®¹è¯ï¼Œå¦‚ï¼šextremely important, very happy, quite difficult
        (('NN', 'VB'), 'Noun+Verb'),  # åè¯+åŠ¨è¯ï¼Œå¦‚ï¼šproblem solving, decision making, time management
        (('VB', 'PRP'), 'Verb+Pronoun'),  # åŠ¨è¯+ä»£è¯ï¼Œå¦‚ï¼šhelp me, tell them, ask us
        (('IN', 'JJ', 'NN'), 'Prep+Adj+Noun'),  # ä»‹è¯+å½¢å®¹è¯+åè¯ï¼Œå¦‚ï¼šin great detail, with special care, on important matters
        (('DT', 'NN', 'IN', 'NN'), 'Det+Noun+Prep+Noun'), # é™å®šè¯+åè¯+ä»‹è¯+åè¯ï¼Œå¦‚ï¼šthe end of time, a piece of cake, the beginning of history
        (('MD', 'VB', 'RB'), 'Modal+Verb+Adv'), # æƒ…æ€åŠ¨è¯+åŠ¨è¯+å‰¯è¯ï¼Œå¦‚ï¼šcan easily do, will quickly go, should carefully consider
        (('NN', 'IN', 'DT', 'NN'), 'Noun+Prep+Det+Noun'), # åè¯+ä»‹è¯+é™å®šè¯+åè¯ï¼Œå¦‚ï¼štransition to a new, solution to the problem, key to a mystery
        (('VB', 'TO', 'VB'), 'Verb+To+Verb'),  # åŠ¨è¯+ä¸å®šå¼æ ‡è®°+åŠ¨è¯ï¼Œå¦‚ï¼šwant to go, need to see, try to understand
        (('VBG', 'NN'), 'Gerund+Noun'),  # åŠ¨åè¯+åè¯ï¼Œå¦‚ï¼šreading books, making progress, writing letters
        (('VBN', 'IN'), 'PastPart+Prep'),  # è¿‡å»åˆ†è¯+ä»‹è¯ï¼Œå¦‚ï¼šinterested in, covered with, known for
        (('CD', 'NNS'), 'Number+PluralNoun'),  # åŸºæ•°è¯+åè¯å¤æ•°ï¼Œå¦‚ï¼šthree books, five years, ten students
        (('JJ', 'CC', 'JJ'), 'Adj+Conj+Adj'),  # å½¢å®¹è¯+è¿è¯+å½¢å®¹è¯ï¼Œå¦‚ï¼šsimple and effective, short but clear, tired yet happy
        (('VB', 'PRP', 'RB'), 'Verb+Pronoun+Adv'),  # åŠ¨è¯+ä»£è¯+å‰¯è¯ï¼Œå¦‚ï¼štell me quickly, show them clearly, ask us politely
        (('RB', 'RB', 'JJ'), 'Adv+Adv+Adj'), # å‰¯è¯+å‰¯è¯+å½¢å®¹è¯ï¼Œå¦‚ï¼švery extremely hot, quite surprisingly good, rather unexpectedly cold
        (('DT', 'JJ', 'NN', 'VBZ'), 'Det+Adj+Noun+Verb'), # é™å®šè¯+å½¢å®¹è¯+åè¯+åŠ¨è¯ï¼Œå¦‚ï¼šthe quick brown fox jumps, a beautiful red rose blooms
        (('PRP', 'MD', 'VB', 'RB'), 'Pron+Modal+Verb+Adv'), # ä»£è¯+æƒ…æ€åŠ¨è¯+åŠ¨è¯+å‰¯è¯ï¼Œå¦‚ï¼šI can easily do, you should carefully consider, we will quickly go
        (('NN', 'VBZ', 'JJ'), 'Noun+Verb+Adj'),  # åè¯+åŠ¨è¯+å½¢å®¹è¯ï¼Œå¦‚: time flies fast, sun sets red, water runs clear
        (('IN', 'PRP$', 'NN'), 'Prep+Possessive+Noun')  # ä»‹è¯+ç‰©ä¸»ä»£è¯+åè¯ï¼Œå¦‚ï¼šin my opinion, on his behalf, with her permission
    ]

    collocation_counts = {desc: Counter() for _, desc in patterns}
    window_size = 4  # æ£€æŸ¥ç›¸é‚»å•è¯çš„çª—å£å¤§å°

    for i in range(int(len(tagged_tokens) - window_size + 1)):
        window = tagged_tokens[i:i + window_size]
        for (pattern, description) in patterns:
            if len(pattern) <= len(window):
                match = True
                for j in range(len(pattern)):
                    if window[j][1] != pattern[j]:
                        match = False
                        break
                if match:
                    phrase = ' '.join([word for word, pos in window[:len(pattern)]])
                    collocation_counts[description][phrase] += 1

    # è·å–æ¯ç§æ¨¡å¼çš„å‰top_nä¸ªæœ€å¸¸è§æ­é…
    top_collocations = {}
    for desc, counter in collocation_counts.items():
        top_collocations[desc] = counter.most_common(top_n)

    return top_collocations


def count_word_frequency(text: str,
                         remove_stopwords: bool = True,
                         min_word_length: int = 2,
                         lemmatize: bool = True) -> Tuple[List[str], Dict[str, int]]:
    """
    ç»Ÿè®¡æ–‡æœ¬ä¸­å•è¯çš„é¢‘ç‡ï¼Œå¹¶è¿›è¡Œè¯¦ç»†çš„é¢„å¤„ç†ã€‚

    Args:
        text (str): è¦åˆ†æçš„æ–‡æœ¬ã€‚
        remove_stopwords (bool): æ˜¯å¦ç§»é™¤åœç”¨è¯ï¼Œé»˜è®¤ä¸º Trueã€‚
        min_word_length (int): å•è¯æœ€å°é•¿åº¦ï¼ŒçŸ­äºæ­¤é•¿åº¦çš„å•è¯å°†è¢«è¿‡æ»¤ï¼Œé»˜è®¤ä¸º 2ã€‚
        lemmatize (bool): æ˜¯å¦è¿›è¡Œè¯å½¢è¿˜åŸï¼Œé»˜è®¤ä¸º Trueã€‚

    Returns:
        Tuple[List[str], Dict[str, int]]: å¥å­åˆ—è¡¨å’Œå•è¯é¢‘ç‡å­—å…¸ã€‚

    Raises:
        ValueError: å½“è¾“å…¥æ–‡æœ¬ä¸ºç©ºæˆ–è¿‡çŸ­æ—¶ã€‚
    """

    # å‚æ•°éªŒè¯
    if not text or not isinstance(text, str):
        raise ValueError("è¾“å…¥æ–‡æœ¬å¿…é¡»æ˜¯éç©ºå­—ç¬¦ä¸²")
    if len(text.strip()) < 10:  # å‡è®¾æ–‡æœ¬è‡³å°‘10ä¸ªå­—ç¬¦
        raise ValueError("è¾“å…¥æ–‡æœ¬è¿‡çŸ­ï¼Œæ— æ³•è¿›è¡Œæœ‰æ„ä¹‰çš„åˆ†æ")

    # 0. å¯é€‰ï¼šåˆæ­¥æ¸…ç†æ–‡æœ¬ï¼ˆç§»é™¤å¤šä½™ç©ºæ ¼ã€æ¢è¡Œç­‰ï¼‰
    clean_text = re.sub(r'\s+', ' ', text.strip())  # å°†å¤šä¸ªç©ºç™½å­—ç¬¦æ›¿æ¢ä¸ºå•ä¸ªç©ºæ ¼

    # 1. åˆ†å¥
    try:
        sentences = sent_tokenize(clean_text)
    except Exception as e:
        raise RuntimeError(f"åˆ†å¥å¤„ç†å¤±è´¥: {str(e)}")

    # åˆå§‹åŒ–å·¥å…·
    stop_words = set(stopwords.words('english')) if remove_stopwords else set()
    lemmatizer = WordNetLemmatizer() if lemmatize else None
    # åˆ›å»ºå»é™¤æ ‡ç‚¹çš„ç¿»è¯‘è¡¨
    translator = str.maketrans('', '', string.punctuation)

    all_words = []

    for sentence in sentences:
        try:
            # 2. åˆ†è¯
            words = word_tokenize(sentence)

            processed_words = []
            for word in words:
                # 2.1 è½¬æ¢ä¸ºå°å†™
                word_lower = word.lower()
                # 2.2 å»é™¤æ ‡ç‚¹ç¬¦å·ï¼ˆä½¿ç”¨translateæ–¹æ³•ï¼Œæ¯”å¾ªç¯åˆ¤æ–­æ•ˆç‡é«˜ï¼‰[10,11](@ref)
                word_no_punct = word_lower.translate(translator)
                # 2.3 æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆå•è¯ï¼ˆé•¿åº¦ã€æ˜¯å¦åŒ…å«æ•°å­—ç­‰ï¼‰
                if not is_valid_word(word_no_punct, min_word_length):
                    continue
                # 2.4 æ£€æŸ¥åœç”¨è¯
                if remove_stopwords and word_no_punct in stop_words:
                    continue

                processed_words.append(word_no_punct)

            # å¦‚æœå½“å‰å¥å­ç»è¿‡è¿‡æ»¤åæ²¡æœ‰è¯ï¼Œåˆ™è·³è¿‡åç»­å¤„ç†
            if not processed_words:
                continue

            # 3. è¯æ€§æ ‡æ³¨ä¸è¯å½¢è¿˜åŸ (å¦‚æœéœ€è¦)
            if lemmatize and lemmatizer:
                # å¯¹å¤„ç†åçš„å•è¯è¿›è¡Œè¯æ€§æ ‡æ³¨ï¼ˆæ³¨æ„ï¼šè¿™é‡Œæ ‡æ³¨çš„æ˜¯åŸå§‹clean wordï¼Œä½†å®é™…ç”¨çš„æ˜¯è½¬å°å†™åçš„ï¼Œç•¥æœ‰è¯¯å·®ä½†å¯æ¥å—ï¼‰
                pos_tags = pos_tag(processed_words) # è¿”å›å½¢å¼å¦‚ [('word', 'tag'), ...]
                final_words = []
                for word, tag in pos_tags:
                    # è·å–WordNetè¯æ€§
                    wn_tag = ptb_to_wn_tag(tag)
                    if wn_tag:
                        # è¿›è¡Œè¯å½¢è¿˜åŸ
                        lemma = lemmatizer.lemmatize(word, pos=wn_tag)
                        final_words.append(lemma)
                    else:
                        final_words.append(word)
            else:
                final_words = processed_words # ä¸è¿›è¡Œè¯å½¢è¿˜åŸ

            all_words.extend(final_words)

        except Exception as e:
            print(f"å¤„ç†å¥å­æ—¶å‡ºé”™: '{sentence}'. é”™è¯¯: {str(e)}")
            traceback.print_exc()
            continue

    # 4. ç»Ÿè®¡è¯é¢‘
    word_freq = Counter(all_words)

    return sentences, dict(word_freq)





# def count_word_frequency(text: str):
#     # 1. åˆ†å¥
#     sentences = sent_tokenize(text)
#
#     # 2. åˆ†è¯ & é¢„å¤„ç†ï¼ˆè½¬æ¢ä¸ºå°å†™ã€å»é™¤æ ‡ç‚¹å’Œåœç”¨è¯ï¼‰
#     stop_words = set(stopwords.words('english'))
#     translator = str.maketrans('', '', string.punctuation)
#     all_words = []
#
#     for sentence in sentences:
#         words = word_tokenize(sentence)
#         # è½¬æ¢ä¸ºå°å†™å¹¶å»é™¤æ ‡ç‚¹ç¬¦å·
#         words = [word.translate(translator).lower() for word in words]
#         # å»é™¤åœç”¨è¯å’Œç©ºå­—ç¬¦ä¸²
#         words = [word for word in words if word not in stop_words and word]
#         all_words.extend(words)
#
#     # 3. è¯å½¢è¿˜åŸ (éœ€è¦å…ˆè¿›è¡Œè¯æ€§æ ‡æ³¨ä»¥è·å¾—æœ€ä½³æ•ˆæœ)
#     # æ³¨æ„ï¼šä¸ºç®€åŒ–ç¤ºä¾‹ï¼Œæˆ‘ä»¬å‡è®¾æ‰€æœ‰è¯éƒ½æ˜¯åè¯('n')ã€‚å®é™…åº”ç”¨ä¸­åº”è¿›è¡Œè¯æ€§æ ‡æ³¨ã€‚
#     lemmatizer = WordNetLemmatizer()
#     lemmatized_words = [lemmatizer.lemmatize(word, pos='v') for word in all_words]  # å°è¯•åŠ¨è¯å½¢å¼
#
#     # 4. ç»Ÿè®¡è¯é¢‘
#     word_freq = Counter(lemmatized_words)
#     return sentences, word_freq


# def analyze_sentence_patterns(sentences):
#     """
#     ä¸€ä¸ªç®€å•åŸºäºè¯æ€§æ ‡ç­¾åºåˆ—çš„å¥å‹ç»Ÿè®¡ç¤ºä¾‹ã€‚
#     è¿™åªæ˜¯ä¸€ä¸ªåˆçº§æ–¹æ³•ï¼Œæ›´å‡†ç¡®çš„å¥å‹åˆ†æéœ€è¦ä¾èµ–å¥æ³•åˆ†æã€‚
#     """
#     # å¸¸è§çš„è¯æ€§æ ‡è®°: NN(åè¯), VB(åŠ¨è¯), IN(ä»‹è¯), DT(å† è¯), JJ(å½¢å®¹è¯), PRP(äººç§°ä»£è¯)
#     pattern_freq = Counter()
#
#     for sentence in sentences:
#         words = word_tokenize(sentence)
#         # è·å–æ¯ä¸ªè¯çš„è¯æ€§æ ‡ç­¾
#         pos_tags = [tag for word, tag in nltk.pos_tag(words)]
#         # å°†è¯æ€§æ ‡ç­¾åºåˆ—è½¬æ¢ä¸ºä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œä½œä¸ºå¥å‹çš„è¿‘ä¼¼è¡¨ç¤º
#         pattern_key = ' '.join(pos_tags)
#         pattern_freq[pattern_key] += 1
#
#         # ä½ ä¹Ÿå¯ä»¥å®šä¹‰ä¸€äº›è§„åˆ™ï¼Œå°†ç‰¹å®šçš„è¯æ€§åºåˆ—æ˜ å°„åˆ°ä½ å®šä¹‰çš„å¥å‹åç§°ä¸Š
#         # if pos_tags starts with 'PRP VB' -> S+V pattern?
#
#     return pattern_freq



# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # directory = "PeppaPig"
    # results = process_all_docx_files(directory)
    #
    # # æŸ¥çœ‹å¤„ç†ç»“æœï¼ˆä¾‹å¦‚ï¼Œæ‰“å°ç¬¬ä¸€ä¸ªæ–‡ä»¶çš„å†…å®¹ï¼‰
    # with open('pure_text.txt', 'wt') as f:
    #     for filename, content in results.items():
    #         print(f"æ–‡ä»¶: {filename}")
    #         print("å¤„ç†åçš„å†…å®¹é¢„è§ˆ:")
    #         print(content[:500] + "..." if len(content) > 500 else content)  # æ‰“å°å‰500å­—ç¬¦
    #         print("\n" + "=" * 50 + "\n")
    #         f.write(content)

    with open('pure_text.txt', 'rt') as f:
        full_text = f.read()

    # ------------------------------------------------------------------------------------------------------------------

    sentences, frequency = count_word_frequency(full_text)

    print("è¯é¢‘ç»Ÿè®¡ç»“æœ (åˆå¹¶å˜å½¢å):")
    for word, count in frequency.most_common(10):  # æ‰“å°æœ€å¸¸è§çš„10ä¸ªè¯
        print(f"{word}: {count}")

    # åˆ›å»ºå¥å­åˆ—è¡¨çš„DataFrame
    df_sentences = pd.DataFrame(sentences, columns=['Sentences'])

    # åˆ›å»ºè¯é¢‘ç»Ÿè®¡çš„DataFrame
    df_frequency = pd.DataFrame(frequency.items(), columns=['Word', 'Frequency'])
    # æŒ‰è¯é¢‘ä»é«˜åˆ°ä½æ’åº
    df_frequency.sort_values(by='Frequency', ascending=False, inplace=True)

    with pd.ExcelWriter('text_analysis_results.xlsx', engine='openpyxl') as writer:
        df_sentences.to_excel(writer, sheet_name='Sentences', index=False)
        df_frequency.to_excel(writer, sheet_name='Word Frequency', index=False)

    print("åˆ†æç»“æœå·²æˆåŠŸå¯¼å‡ºåˆ° 'text_analysis_results.xlsx'")

    # ------------------------------------------------------------------------------------------------------------------

    collocations = analyze_collocations(full_text)

    print("\n=== å¸¸è§è¡¨è¾¾æ–¹å¼å½’ç±» ===")
    for pattern_type, phrases in collocations.items():
        if phrases:  # åªæ˜¾ç¤ºæœ‰ç»“æœçš„ç±»å‹
            print(f"\n--- {pattern_type} ---")
            for phrase, freq in phrases:
                print(f"{phrase}: {freq}")

    data = []
    for collocation_type, phrases_list in collocations.items():
        for phrase, frequency in phrases_list:
            data.append({
                "æ­é…æ¨¡å¼": collocation_type,
                "æ­é…çŸ­è¯­": phrase,
                "å‡ºç°é¢‘ç‡": frequency
            })

    df_collocations = pd.DataFrame(data)

    # 3. å†™å…¥ Excel æ–‡ä»¶
    excel_filename = "collocation_analysis_results.xlsx"
    df_collocations.to_excel(excel_filename, index=False)

    print(f"æ­é…åˆ†æç»“æœå·²ä¿å­˜åˆ° '{excel_filename}'")


# ----------------------------------------------------------------------------------------------------------------------

def demo_remove_non_english():
    # æµ‹è¯•ç”¨ä¾‹
    test_text = "Hello, ä½ å¥½ï¼ This is a test. 123 456 ğŸ‰"

    # ç¤ºä¾‹ 1: é»˜è®¤æ¨¡å¼ï¼ˆåªä¿ç•™å­—æ¯å’Œç©ºæ ¼ï¼‰
    result1 = remove_non_english(test_text)
    print("é»˜è®¤æ¨¡å¼ (ä¿ç•™å­—æ¯å’Œç©ºæ ¼):", result1)  # è¾“å‡º: "Hello  This is a test  "

    # ç¤ºä¾‹ 2: ä¿ç•™å­—æ¯å’Œæ•°å­—
    result2 = remove_non_english(test_text, keep_space=False, keep_number=True)
    print("ä¿ç•™å­—æ¯å’Œæ•°å­—:", result2)  # è¾“å‡º: "HelloThisisatest123456"

    # ç¤ºä¾‹ 3: åªä¿ç•™è‹±æ–‡å­—æ¯
    result3 = remove_non_english(test_text, keep_space=False, keep_number=False)
    print("åªä¿ç•™è‹±æ–‡å­—æ¯:", result3)  # è¾“å‡º: "HelloThisisatest"

    # ç¤ºä¾‹ 4: ä¿ç•™å­—æ¯ã€æ•°å­—å’Œç©ºæ ¼
    result4 = remove_non_english(test_text, keep_space=True, keep_number=True)
    print("ä¿ç•™å­—æ¯ã€æ•°å­—å’Œç©ºæ ¼:", result4)  # è¾“å‡º: "Hello  This is a test. 123 456 "


def main():
    demo_remove_non_english()


if __name__ == '__main__':
    try:
        main()
    except Exception as e:
        print(str(e))
        traceback.print_exc()
    finally:
        pass
